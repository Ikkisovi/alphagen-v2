# Ensemble Alpha Mining Configuration
# This config defines parameters for multi-window ensemble factor training

# ============================================================================
# TIME WINDOWS
# ============================================================================
time_windows:
  # Training windows (aligned to trading calendar)
  train_12m:
    name: "12m"
    start_date: "2023-01-01"
    end_date: "2023-12-31"
    description: "12-month historical window"

  train_6m:
    name: "6m"
    start_date: "2023-07-01"
    end_date: "2023-12-31"
    description: "6-month medium-term window"

  train_3m:
    name: "3m"
    start_date: "2023-10-01"
    end_date: "2023-12-31"
    description: "3-month short-term window"

  # Validation window for final ensemble optimization
  validation:
    start_date: "2024-01-01"
    end_date: "2024-03-31"
    description: "Q1 2024 validation period (matches 2024_01 deliverable)"

# ============================================================================
# DATA SOURCE CONFIGURATION
# ============================================================================
data:
  source: "feature_store"
  path: "E:/factor/feature_store/am_pm_features"
  forward_horizon: 20
  sessions:
    - "AM"
    - "PM"
  timezone: "UTC"
  features:
    - open
    - high
    - low
    - close
    - volume
    - vwap
    - market_cap
    - turnover
  price_features:
    - open
    - high
    - low
    - close
    - volume
    - vwap
    - market_cap
    - turnover
  feature_patterns:
    - open
    - high
    - low
    - close
    - volume
    - vwap
    - "return*"
    - "ret_*"
    - "rel_ret_*"
    - "TE_*"
    - "CORR_*"
  derived_feature_patterns:
    - "return*"
    - "ret_*"
    - "rel_ret_*"
    - "TE_*"
    - "CORR_*"
  preload:
    start: "2022-01-01"
    end: "2024-03-31"
  max_backtrack_days: 240
  max_future_days: 120

# ============================================================================
# UNIVERSE CONFIGURATION
# ============================================================================
universe:
  base_instrument: "csi300"  # Base universe (csi300, csi500, or custom)

  # Additional stocks for tech exposure
  additional_stocks:
    - "AAPL"   # Apple
    - "GOOGL"  # Google/Alphabet
    - "AMZN"   # Amazon
    - "NVDA"   # NVIDIA
    - "SNDK"   # SanDisk (or WDC if merged)

  # Universe fingerprint for cache validation
  fingerprint_file: "output/universe_fingerprint.json"

# ============================================================================
# TRAINING CONFIGURATION
# ============================================================================
training:
  # Single-stage technical training
  stage1_technical:
    enabled: true
    max_episodes: 500
    min_episodes: 400
    early_stopping_patience: 30  # Stop after 30 flat episodes
    target_candidates: 80        # Target ~80 candidate factors

  # Pool configuration per window
  pool_capacity_per_window: 25

  # PPO/RL hyperparameters
  ppo:
    learning_rate: 0.0001
    gamma: 0.99
    gae_lambda: 0.95
    clip_epsilon: 0.2
    entropy_coef: 0.01
    value_loss_coef: 0.5

  reward:
    objective: "IC + ICIR - turnover_penalty"
    turnover_penalty_coeff: 0.05

  # Device configuration
  device: "cuda:0"  # Use GPU if available, fallback to CPU

# ============================================================================
# ENSEMBLE CONFIGURATION
# ============================================================================
ensemble:
  # Merging settings
  final_capacity: 45               # Maximum factors in final ensemble
  auto_prune_weak_factors: true    # Let optimizer drop weak contributors

  # Optimization settings
  optimizer:
    learning_rate: 0.0005
    max_steps: 10000
    tolerance: 500
    l1_alpha: 0.005                # L1 regularization strength

  # Cross-validation shrinkage
  cross_validation:
    enabled: true
    n_folds: 5
    shrinkage_factor: 0.9          # Weight shrinkage to reduce overfitting

  # IC thresholds
  ic_lower_bound: 0.01             # Minimum IC for factor inclusion

# ============================================================================
# CACHE MANAGEMENT
# ============================================================================
cache:
  enabled: true
  freshness_days: 30               # Reuse pools if newer than 30 days
  check_universe_consistency: true # Validate universe hasn't changed
  cache_dir: "output/cache"
  skip_training_if_cached: false   # Manual override to skip training

  # Cache file patterns
  pool_file_pattern: "pool_{window}.json"
  ensemble_file_pattern: "ensemble_pool_{validation_start}.json"

# ============================================================================
# OUTPUT CONFIGURATION
# ============================================================================
output:
  base_dir: "output/ensemble"

  # Pool outputs per window
  pool_12m: "output/ensemble/pool_12m.json"
  pool_6m: "output/ensemble/pool_6m.json"
  pool_3m: "output/ensemble/pool_3m.json"

  # Final ensemble output
  final_ensemble: "output/ensemble/ensemble_pool_2024Q1.json"

  # Logs and reports
  training_log: "output/ensemble/ensemble_training.log"
  performance_report: "output/ensemble/ensemble_report_2024Q1.txt"

  # Detailed metrics
  save_detailed_metrics: true
  metrics_file: "output/ensemble/ensemble_metrics_2024Q1.json"

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  console_output: true
  file_output: true
